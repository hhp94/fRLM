for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Compile the array of exposures
basis <- getBasis( L, grid )
L <- 5
# Compile the array of exposures
basis <- getBasis( L, grid )
Int_XtimesBasis <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
Int_XtimesBasis <- Int_XtimesBasis %>% unlist %>% array(dim = c(n, L, p))
# get them in a n x L x p array
Int_XtimesBasis <- Int_XtimesBasis %>% unlist %>% array(dim = c(n, L, length(Int_XtimesBasis)))
Int_XtimesBasis <- lapply(condMu, function(X) X %*% basis / ncol(X))
Int_XtimesBasis
Int_XtimesBasis %>% unlist
Int_XtimesBasis %>% unlist %>% array(dim = c(n, L, length(Int_XtimesBasis)))
# get them in a n x L x p array
Int_XtimesBasis <- Int_XtimesBasis %>% unlist %>% array(dim = c(n, L, length(exposures)))
n
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
Int_XtimesBasis <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
Int_XtimesBasis <- Int_XtimesBasis %>% unlist %>% array(dim=c(dim$n, dim$L, dim$p))
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
C
stanFile <- paste0("inst/stan/", fileName)
fileName <- "fRLM_additive.stan"
stanFile <- system.file("stan", fileName, package = "fRLM")
stanFile <- paste0("inst/stan/", fileName)
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$n, dim$L, dim$p))
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
# Stan: Bayesian regression
# -------------------------
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par,
))
alpha_par <- 1
beta_par <- 1
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$n, dim$L, dim$p))
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
# Stan: Bayesian regression
# -------------------------
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par,
))
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par
))
data_stan
fileName <- "fRLM_additive.stan"
stanFile <- system.file("stan", fileName, package = "fRLM")
stanFile <- paste0("inst/stan/", fileName)
fit <- rstan::stan( file = stanFile, data = data_stan, ... )
stanFile <- paste0("inst/stan/", fileName)
fit <- rstan::stan( file = stanFile, data = data_stan)
data <- data %>% mutate(exposure2 = exposure + rnorm(length(exposure)))
data
exposures <- c("exposure", "exposure2")
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$n, dim$L, dim$p))
eta <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$p, dim$n, dim$L))
eta
dim(eta)
eta[[1]]
eta[1]
eta[1,,]
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a n x L x p array
eta <- eta_list %>% unlist %>% array(dim=c(dim$p, dim$n, dim$L))
eta_list[[1]]
eta[1,,]
all.equal(eta_list[[1]], eta[1,,])
?array
# get them in a n x L x p array
eta <- eta_list %>% array(dim=c(dim$p, dim$n, dim$L))
eta
eta[1]
eta[1,,]
eta <- do.call(cbind(eta_list))
eta <- do.call(cbind, eta_list)
eta
flatten(eta)
# get them in a n x L x p array
eta <- eta %>% as.vector %>% array(dim=c(dim$p, dim$n, dim$L))
eta[1,,]
eta_list[[1]]
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
eta <- do.call(rbind, eta_list)
eta <- lapply(eta_list, function(x) as.vector(x))
eta
eta <- lapply(eta_list, function(x) as.vector(x))
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
eta <- lapply(eta_list, function(x) as.vector(x))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$p, dim$n, dim$L))
eta[1,,]
eta_list[[1]]
eta <- lapply(eta_list, function(x) as.vector(t(x)))
# get them in a n x L x p array
eta <- eta %>% unlist %>% array(dim=c(dim$p, dim$n, dim$L))
eta[1,,]
eta_list[[1]]
library(abind)
install.packages("abind")
library(abind)
eta <- abind(eta_list, along=1)
eta
eta[1,,]
length(eta_list)
?abind
dim(eta)
eta <- abind(eta_list, along=0)
dim(eta)
eta[1,,]
eta_list[[1]]
eta_list[[1]]
eta[1,,]
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a p x n x L array
eta <- abind(eta_list, along=0)
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par
))
fileName <- "fRLM_additive.stan"
stanFile <- system.file("stan", fileName, package = "fRLM")
fit <- rstan::stan( file = stanFile, data = data_stan, ...)
stanFile <- paste0("inst/stan/", fileName)
fit <- rstan::stan( file = stanFile, data = data_stan)
exposures <- c("exposure")
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a p x n x L array
eta <- abind(eta_list, along=0)
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
# Stan: Bayesian regression
# -------------------------
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par
))
fileName <- "fRLM_additive.stan"
stanFile <- system.file("stan", fileName, package = "fRLM")
fit <- rstan::stan( file = stanFile, data = data_stan, ...)
stanFile <- paste0("inst/stan/", fileName)
fit <- rstan::stan( file = stanFile, data = data_stan)
fit$sigma
fit
rstan::extract(fit, 'alpha')
rstan::extract(fit, 'beta')
a <- rstan::extract(fit, 'beta')
a
dim(a$beta)
a
length8a
length(a)
a
# output:
alpha <- rstan::extract(fit, 'alpha')[[1]]
beta  <- rstan::extract(fit, 'beta')[[1]]
delta <- rstan::extract(fit, 'delta')[[1]]
sigma <- rstan::extract(fit, 'sigma')[[1]]
dim(sigma)
sigma
length(sigma)
exposures <- c("exposure", "exposure2")
exposures
fit <- rstan::stan( file = stanFile, data = data_stan)
# Create the time scaler. This function scales the data down and back to the original scale
timeScaler <- timeScaler_ff(data %>% pull(!!sym(time)), min=0.05, max=0.95)
# Standardize time
data <- data %>% mutate(!!time := timeScaler(data[[time]]))
# Define the grid
grid <- seq(0,1, l = 150 )
# Fit the gaussian processes for each exposure
condMu <- list()
for (exposure in exposures) {
# extract outcome for each exposure
subdata <- data %>% filter(!is.na({{exposure}}))
t_obs <- subdata %>% group_by(!!sym(id)) %>% summarise(t_obs = list(!!sym(time))) %>% pull(t_obs)
expo  <- subdata %>% group_by(!!sym(id)) %>% summarise(expo = list(!!sym(exposure))) %>% pull(expo)
gpfitList <- lapply( 1:length(t_obs), function(i) gpFit( t_obs[[i]], expo[[i]] ) )
condMu[[exposure]] <- t( sapply( gpfitList, predict, tnew = grid ) )
}
# Extract quantities
# ------------------
# Extract y
y <- data %>% group_by(!!sym(id)) %>% summarise(outcome = mean(!!sym(outcome))) %>% pull(outcome)
# Declare the dimensions
dim <- list(
"n" = length(y),
"L" = L,
"p" = length(exposures),
"d" = length(controls) + 1
)
# Compile the array of exposures
basis <- getBasis( L, grid )
eta_list <- lapply(condMu, function(X) X %*% basis / ncol(X))
# get them in a p x n x L array
eta <- abind(eta_list, along=0)
# Compile the controls
C <- data %>% group_by(!!sym(id)) %>% summarise(across(all_of(controls), ~mean(.))) %>% ungroup() %>%
# select the controls
select(all_of(controls)) %>%
# add intercept
mutate(intercept=1, .before=1) %>% as.matrix
# Stan: Bayesian regression
# -------------------------
data_stan <- c(dim, list(
y = y,
eta = eta,
C = C,
alpha_par = alpha_par,
beta_par = beta_par
))
fileName <- "fRLM_additive.stan"
stanFile <- system.file("stan", fileName, package = "fRLM")
fit <- rstan::stan( file = stanFile, data = data_stan, ...)
stanFile <- paste0("inst/stan/", fileName)
fit <- rstan::stan( file = stanFile, data = data_stan)
# output:
alpha <- rstan::extract(fit, 'alpha')[[1]]
beta  <- rstan::extract(fit, 'beta')[[1]]
delta <- rstan::extract(fit, 'delta')[[1]]
sigma <- rstan::extract(fit, 'sigma')[[1]]
alpha
beta
dim(beta)
dim(alpha)
dim(delta)
dim(sigma)
data(toy)
toy
library(fRLM)
toy$
d
devtools::document()
roxygen2::roxygenize()
rm(timeScaler_ff)
roxygen2::roxygenize()
library(fRLM)
?fRLM
library(fRLM)
data(toy)
output <- fRLM(data=toy,
id = "id",
time="age",
exposures="exposure",
outcome="outcome",
warmup = 500, iter = 1000, chains = 2) # this is passed to stan
plot(output)
output
output
library(fRLM)
data(toy)
output <- fRLM(data=toy,
id = "id",
time="age",
exposures="exposure",
outcome="outcome") # this is passed to stan
library(fRLM)
data(toy)
fRLM(data=toy,
id = "id",
time="age",
exposures="exposure",
outcome="outcome",
warmup = 500, iter = 10000, chains = 2) # this is passed to stan
